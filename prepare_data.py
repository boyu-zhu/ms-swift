import zipfile
from huggingface_hub import hf_hub_download
from datasets import load_dataset, DownloadConfig
import os
from tqdm import tqdm
import requests
from huggingface_hub import hf_hub_download
import os
import zipfile
import tarfile
import gdown
import boto3
import os
import os
# import boto3
from botocore.exceptions import NoCredentialsError, ClientError
import argparse

def download_files(bucket_name, keys, local_dir):
    """
    ä» S3 æ¡¶ä¸­ä¸‹è½½ä¸€ç³»åˆ—æ–‡ä»¶åˆ°æœ¬åœ°ç›®å½•ã€‚

    å‚æ•°ï¼š
    - bucket_name: S3 æ¡¶åï¼ˆstringï¼‰
    - keys: è¦ä¸‹è½½çš„ S3 å¯¹è±¡ key åˆ—è¡¨ï¼ˆlist of stringsï¼‰
    - local_dir: æœ¬åœ°å­˜æ”¾ç›®å½•ï¼ˆstringï¼‰
    """
    s3 = boto3.client('s3')
    os.makedirs(local_dir, exist_ok=True)

    for key in keys:
        filename = os.path.basename(key)
        local_path = os.path.join(local_dir, filename)
        try:
            print(f"Downloading s3://{bucket_name}/{key} â†’ {local_path}")
            s3.download_file(bucket_name, key, local_path)
        except NoCredentialsError:
            print("Error: AWS credentials not found.")
            return
        except ClientError as e:
            print(f"Error downloading {key}: {e}")
        else:
            print(f"Successfully downloaded {filename}")



HF_TOKEN = os.getenv("HF_TOKEN")

def prepare_vlguard():
    zip_path = hf_hub_download(
        repo_id="ys-zong/VLGuard",
        filename="train.zip",
        repo_type="dataset",
        token=HF_TOKEN,
        local_dir="./data",               # â­ ä¸‹è½½åˆ°æŒ‡å®šç›®å½•
        local_dir_use_symlinks=False         # â­ å¤åˆ¶è€Œä¸æ˜¯è½¯é“¾æ¥ï¼ˆé¿å…ä»ç„¶æŒ‡å‘ç¼“å­˜ï¼‰
    )
    print("ä¸‹è½½è·¯å¾„:", zip_path)

    # è§£å‹
    extract_dir = "./data/vlguard"
    os.makedirs(extract_dir, exist_ok=True)

    with zipfile.ZipFile(zip_path, "r") as zip_ref:
        zip_ref.extractall(extract_dir)

    print("è§£å‹å®Œæˆ:", extract_dir)

def prepare_vlsbench():
    dataset = load_dataset("Foreshhh/vlsbench", split='train', token=HF_TOKEN)
    for item in dataset:
        image = item.get("image")
        path = item.get("image_path")
        image_path = os.path.join('/root/ms-swift/data/vlsbench', path)
        if image and path:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(image_path), exist_ok=True)
            try:
                image.save(image_path)
                print(f"âœ… Saved image to {path}")
            except Exception as e:
                print(f"âŒ Failed to save {path}: {e}")

def prepare_llavaguard():
    hf_token = HF_TOKEN  # ä½ çš„ token
    set_name = "train"
    save_dir = "data/llavaguard/train"

    if os.path.exists(save_dir) and len(os.listdir(save_dir)) > 0:
        print(f"[Info] Folder {save_dir} already exists, skip downloading.")
        return
    os.makedirs(save_dir, exist_ok=True)

    dataset = load_dataset("AIML-TUDA/LlavaGuard", token=hf_token)
    urls = dataset[set_name]["url"]

    headers = {"User-Agent": "Mozilla/5.0"}
    auth_headers = {"Authorization": f"Bearer {hf_token}", "User-Agent": "Mozilla/5.0"}

    for i, url in tqdm(enumerate(urls), total=len(urls)):
        file_path = f"{save_dir}/{i}.jpg"
        if os.path.exists(file_path):
            # æ–‡ä»¶å·²å­˜åœ¨ -> è·³è¿‡
            continue

        try:
            if "huggingface" in url:
                r = requests.get(url, headers=auth_headers, timeout=(5, 30))
            else:
                r = requests.get(url, headers=headers, timeout=(5, 30))
            r.raise_for_status()

            with open(file_path, "wb") as f:
                f.write(r.content)

        except Exception as e:
            print(f"[Warning] Failed to download {url} -> {e}")

def prepare_safesora():
    zip_path = hf_hub_download(
        repo_id="PKU-Alignment/SafeSora-Label",
        filename="videos.tar.gz",
        repo_type="dataset",
        token="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",
        local_dir="./data",               # â­ ä¸‹è½½åˆ°æŒ‡å®šç›®å½•
        local_dir_use_symlinks=False      # â­ å¤åˆ¶è€Œä¸æ˜¯è½¯é“¾æ¥ï¼ˆé¿å…ä»ç„¶æŒ‡å‘ç¼“å­˜ï¼‰
    )
    print("ä¸‹è½½è·¯å¾„:", zip_path)

    # è§£å‹ç›®å½•
    extract_dir = "./data/safesora"
    os.makedirs(extract_dir, exist_ok=True)

    # â­ å¦‚æœç›®å½•éç©ºï¼Œç›´æ¥è·³è¿‡è§£å‹
    if os.path.exists(extract_dir) and len(os.listdir(extract_dir)) > 0:
        print(f"[Info] æ–‡ä»¶å¤¹ {extract_dir} å·²å­˜åœ¨ä¸”éç©ºï¼Œè·³è¿‡è§£å‹ã€‚")
        return extract_dir

    # å¦åˆ™æ‰§è¡Œè§£å‹
    with tarfile.open(zip_path, "r:gz") as tar_ref:
        tar_ref.extractall(extract_dir)

    print("è§£å‹å®Œæˆ:", extract_dir)



def prepare_text_datasets():
    bucket = 'orby-ucd'
    keys = [
        'sft/Aegis_w_gt_sft.jsonl',
        'sft/beaver_w_gt_sft.jsonl',
        'sft/toxic_w_gt_sft.jsonl',
        'sft/wildguardmix_w_gt_sft.jsonl'
    ]
    target_dir = './data/text_ready_sft_data'  # ä¿®æ”¹ä¸ºä½ æƒ³ä¿å­˜çš„æœ¬åœ°ç›®å½•ï¼Œæ¯”å¦‚ "/ms-swift/downloaded_sft"
    download_files(bucket, keys, target_dir)

def prepare_audio_datasets():
    bucket = 'orby-ucd'
    keys = [
        'sft/Aegis_w_gt_sft.jsonl',
        'sft/beaver_w_gt_sft.jsonl',
        'sft/toxic_w_gt_sft.jsonl',
        'sft/wildguardmix_w_gt_sft.jsonl'
    ]
    target_dir = './data/text_ready_sft_data'  # ä¿®æ”¹ä¸ºä½ æƒ³ä¿å­˜çš„æœ¬åœ°ç›®å½•ï¼Œæ¯”å¦‚ "/ms-swift/downloaded_sft"
    download_files(bucket, keys, target_dir)

def prepare_tocix_chat_audio():
    bucket = 'orby-ucd'
    keys = [
        'data/toxic_chat_audio.tar.gz',
        'sft/toxic_chat_audio.jsonl',

    ]
    target_dir = './data/audio'  # ä¿®æ”¹ä¸ºä½ æƒ³ä¿å­˜çš„æœ¬åœ°ç›®å½•ï¼Œæ¯”å¦‚ "/ms-swift/downloaded_sft"
    # download_files(bucket, keys, target_dir)
    os.makedirs(target_dir, exist_ok=True)

    # ä¸‹è½½æ–‡ä»¶
    download_files(bucket, keys, target_dir)

    # æ‰¾åˆ°åˆšä¸‹è½½çš„ tar.gz æ–‡ä»¶è·¯å¾„
    tar_filename = os.path.basename(keys[0])  # "toxic_chat_audio.tar.gz"
    tar_path = os.path.join(target_dir, tar_filename)

    # è§£å‹ tar.gz åˆ°æŒ‡å®šç›®å½•
    # å‡è®¾ä½ æƒ³æŠŠå®ƒè§£å‹åˆ° `./data/audio/extracted`ï¼Œä½ å¯ä»¥æ”¹ç›®å½•
    extract_dir = os.path.join(target_dir, "toxic_chat_audio")
    os.makedirs(extract_dir, exist_ok=True)

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å†è§£å‹
    if os.path.isfile(tar_path):
        try:
            print(f"Extracting {tar_path} â†’ {extract_dir}")
            with tarfile.open(tar_path, mode="r:gz") as tar_ref:
                tar_ref.extractall(path=extract_dir)
            print("Extraction complete.")
        except tarfile.TarError as e:
            print(f"Error extracting tar file: {e}")
    else:
        print(f"Tar file not found: {tar_path}")


def prepare_fakesv():
    # è¦ä¸‹è½½çš„æ–‡ä»¶ ID å’Œè¾“å‡ºæ–‡ä»¶å
    drive_files = [
        ("1-W_QHoKczSB-DJ4YzkO35PBK9uSEG1dL", "complete.jsonl"),
        ("1-Wfru9llIW8EloZ5RHoOuTjVKQAOpvkr", "videos.zip")
    ]
    
    # ä¸‹è½½ç›®å½•
    download_dir = "./downloads"
    os.makedirs(download_dir, exist_ok=True)
    
    for file_id, filename in drive_files:
        output_path = os.path.join(download_dir, filename)
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨å°±è·³è¿‡ä¸‹è½½
        if os.path.exists(output_path):
            print(f"âš¡ å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ï¼š{output_path}")
        else:
            print(f"â¬‡ æ­£åœ¨ä¸‹è½½ {filename} â€¦")
            # fuzzy=True è®© gdown èƒ½è§£ææ ‡å‡†åˆ†äº«é“¾æ¥
            gdown.download(id=file_id, output=output_path, fuzzy=True)
        
        # å¦‚æœæ˜¯ zip æ–‡ä»¶ï¼Œå°±è§£å‹
        if filename.lower().endswith(".zip"):
            extract_to = "./data/fakesv"
            os.makedirs(extract_to, exist_ok=True)
            print(f"ğŸ“¦ è§£å‹ {filename} åˆ° {extract_to}")
            with zipfile.ZipFile(output_path, 'r') as zf:
                zf.extractall(extract_to)
            print(f"âœ… è§£å‹å®Œæˆï¼š{extract_to}")



def main(mode):
    """
    mode: 'text', 'image', æˆ– 'all'
    """
    if mode in ('image', 'all'):
        print("=== Preparing image datasets ===")
        prepare_vlguard()
        prepare_vlsbench()
        prepare_llavaguard()
        pre()
    if mode in ('text', 'all'):
        print("=== Preparing text datasets ===")
        prepare_text_datasets()
    if mode in ('video', 'all'):
        print("=== Preparing image datasets ===")
        # prepare_safesora()
        prepare_fakesv()
    if mode in ('audio', 'all'):
        print("=== Preparing image datasets ===")
        prepare_tocix_chat_audio()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å‡†å¤‡ dataset è„šæœ¬")
    parser.add_argument(
        "--mode",
        choices=["text", "image", "video", "audio", "all"],
        default="all",
        help="é€‰æ‹©è¦å‡†å¤‡çš„èµ„æºï¼štext, image, æˆ– allï¼ˆé»˜è®¤ï¼‰"
    )
    args = parser.parse_args()

    print(f"è¿è¡Œæ¨¡å¼: {args.mode}")
    main(args.mode)